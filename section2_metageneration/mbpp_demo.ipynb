{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f51599-188f-40d0-a13f-bb0c360b4179",
   "metadata": {},
   "source": [
    "## Meta-generation case study: Python code generation on MBPP\n",
    "\n",
    "This notebook will demonstrate __best-of-n__,  __minimum Bayes risk decoding__, and __self-repair__ for code generation on the Mostly Basic Python Problems (MBPP) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd8829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from litellm import completion, ModelResponse\n",
    "import datasets\n",
    "import jellyfish # levenshtein distance for MBR utility\n",
    "import numpy as np\n",
    "\n",
    "from typing import Callable, Union\n",
    "from pprint import pprint\n",
    "\n",
    "from mbpp_utils import (\n",
    "    # code execution helpers\n",
    "    check_correctness,\n",
    "    execute_tests,\n",
    "    execute_codes,\n",
    "    \n",
    "    # string processing helpers\n",
    "    make_prompt,\n",
    "    extract_code,\n",
    "    extract_func_calls,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2285d-ae3d-45bb-b9c9-157bdf63e7ed",
   "metadata": {},
   "source": [
    "First, let's load in the MBPP data. MBPP consists of basic algorithmic Python questions, such as the following:\n",
    "\n",
    "```python\n",
    "# Input: Specification\n",
    "\"\"\"\n",
    "Write a python function to remove first and last occurrence of a given character from the string.\n",
    "\"\"\"\n",
    "\n",
    "# Output: Python function\n",
    "def remove_Occ(s,ch): \n",
    "    for i in range(len(s)): \n",
    "        if (s[i] == ch): \n",
    "            s = s[0 : i] + s[i + 1:] \n",
    "            break\n",
    "    for i in range(len(s) - 1,-1,-1):  \n",
    "        if (s[i] == ch): \n",
    "            s = s[0 : i] + s[i + 1:] \n",
    "            break\n",
    "    return s \n",
    "\n",
    "# Public test cases\n",
    "assert remove_Occ(\"hello\",\"l\") == \"heo\"\n",
    "assert remove_Occ(\"abcda\",\"a\") == \"bcd\"\n",
    "assert remove_Occ(\"PHP\",\"P\") == \"H\"\n",
    "\n",
    "# Challenge test cases\n",
    "assert remove_Occ(\"hellolloll\",\"l\") == \"helollol\"\n",
    "assert remove_Occ(\"\",\"l\") == \"\"\n",
    "```\n",
    "\n",
    "For each example, there will be a public set of test cases that the model will have access to and a challenge set that will be hidden from the model.\n",
    "\n",
    "We'll evaluate outputs based on their execution accuracy on the combination of public and challenge tests. For sampling-based methods, we'll consider the average execution accuracy across all samples (as there is no ranking over samples), while for meta-decoding methods, we'll consider only the execution accuracy of the top-1 returned output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db06b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mbpp data\n",
    "mbpp = datasets.load_dataset(\"mbpp\", split=\"test\")\n",
    "\n",
    "# pprint(\"MBPP example:\")\n",
    "# pprint(mbpp[0])\n",
    "# print()\n",
    "\n",
    "prompts = []\n",
    "\n",
    "# make zero shot prompts for MBPP\n",
    "for example in mbpp:\n",
    "    prompts.append(make_prompt(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b681268-992e-4adf-a2ff-3ade4fd89a1e",
   "metadata": {},
   "source": [
    "### Parallel Sampling\n",
    "\n",
    "First, consider simply sampling a set of outputs from the model in parallel, using temperature & top-$p$ sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a232d434-e494-4cbc-be15-104ceac5e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set some sampling parameters that we'll keep fixed throughtout this demo\n",
    "n = 30             # number of outputs to sample per input\n",
    "temperature = 0.7  # sampling temperature\n",
    "top_p = 0.95       # top-p cutoff\n",
    "RANDOM_SEED = 1618 # fixed random seed to avoid nondeterminism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8836a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for sampling from generator\n",
    "\n",
    "MODEL_NAME = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "def generate_code(prompt: Union[str, list[dict]], **generate_kwargs) -> tuple[list[str], ModelResponse]:\n",
    "    '''\n",
    "    Generates code sample(s) for prompt\n",
    "    Returns list of code samples and OpenAI-like response object.\n",
    "    \n",
    "    This function accepts additional keyword arguments for various generation \n",
    "    parameters supported by the openai api, including temperature, top_p, logprobs, etc.\n",
    "\n",
    "    Note that we use the litellm library for model inference, which is compatible with a \n",
    "    wide array of model frameworks and APIs. See this link for more:\n",
    "    https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs.\n",
    "    While we assume the inference provider is OpenAI, the code in this demo could \n",
    "    also work for other backends with some minor modifications.\n",
    "    '''\n",
    "    if isinstance(prompt, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    else:\n",
    "        assert isinstance(prompt, list) and isinstance(prompt[0], dict)\n",
    "        messages = prompt\n",
    "\n",
    "    response = completion(MODEL_NAME, messages=messages, **generate_kwargs)\n",
    "\n",
    "    # post-processing to extract code from chat model outputs\n",
    "    codes = []\n",
    "    for choice in response.choices:\n",
    "        text = choice.message.content\n",
    "        code = extract_code(text)\n",
    "        codes.append(code)\n",
    "    return codes, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63e257eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do parallel sampling\n",
    "codes, response = generate_code(prompts[0], n=n, temperature=temperature, top_p=top_p, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2c2e988-876f-429d-ad2f-72cbaea1011d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean pass@1: 0.5333333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'failed: substring not found',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate generated samples\n",
    "execution_results = execute_tests(codes, mbpp[0]['test_list'] + mbpp[0]['challenge_test_list'])\n",
    "print(\"mean pass@1:\", sum(result['passed'] for result in execution_results) / len(execution_results))\n",
    "[result['result'] for result in execution_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3272e29c-7069-4321-bb37-64317ca10fd1",
   "metadata": {},
   "source": [
    "Of the 30 samples, only 16 pass all the tests. This isn't great -- if we were to randomly sample a program, there would only be a slightly over 50% chance of it being correct :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215866b",
   "metadata": {},
   "source": [
    "### Best-of-$n$\n",
    "\n",
    "$$\\hat{y} = \\arg \\max_{y \\in \\mathcal{Y}} v(y)$$\n",
    "\n",
    "As our first meta-decoding algorithm, we consider best-of-$n$ using a sample's log probability under the generator as its value, i.e. $v(y) = p_\\theta(y\\,|\\,x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74c8508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_of_n_logprob(prompt: str, **generate_kwargs) -> tuple[list[str], list[float]]:\n",
    "    '''\n",
    "    Runs best-of-n generation, using mean sequence logprob as value\n",
    "    Returns list of codes in order of decreasing value\n",
    "    '''\n",
    "    # force these arguments to be passed \n",
    "    generate_kwargs[\"top_logprobs\"] = 1\n",
    "    generate_kwargs[\"logprobs\"] = True\n",
    "    # generate samples\n",
    "    codes, response = generate_code(prompt, **generate_kwargs)\n",
    "    scores = []\n",
    "    # compute mean logprob for each sequence\n",
    "    for choice in response.choices:\n",
    "        logprobs = choice.logprobs['content']\n",
    "        mean_logprob = sum(lp['logprob'] for lp in logprobs) / len(logprobs)\n",
    "        scores.append(mean_logprob)\n",
    "\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    # arrange codes by decreasing mean log probability\n",
    "    scores = [scores[i] for i in sorted_indices]\n",
    "    codes = [codes[i] for i in sorted_indices]\n",
    "    return codes, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f08892dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run best-of-n\n",
    "codes, scores = best_of_n_logprob(prompts[0], n=n, temperature=temperature, top_p=top_p, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9f331e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed: substring not found']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate best-of-n\n",
    "execution_results = execute_tests(codes, mbpp[0]['test_list'] + mbpp[0]['challenge_test_list'])\n",
    "[result['result'] for result in execution_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc04eb9-ed8c-42ed-8b95-bf2ef2f16f77",
   "metadata": {},
   "source": [
    "Even still, the top-1 output is not correct -- worse, the top few candidates are all wrong. As it turns out, generator probability has been found to be a relatively poor proxy for execution accuracy; in contrast, recent work has found greater success using mutual information [(Zhang et al., 2022)](https://arxiv.org/abs/2211.16490) or learned reward models [(Ni et al., 2023)](https://arxiv.org/abs/2302.08468) as value functions for best-of-$n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f8699-5fb7-4f8a-b027-39dacd936144",
   "metadata": {},
   "source": [
    "### Minimum Bayes Risk\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y} &= \\arg \\max_{y' \\in \\mathcal{Y_h}} \\sum_{y \\in \\mathcal{Y_e}} u(y, y')\n",
    "\\end{align*}\n",
    "\n",
    "In this section, we consider minimum Bayes risk. We consider two choices of pairwise utility function: (1) character-level edit similarity and (2) code execution equivalence, referred to as MBR-Exec [(Shi et al., 2022)](https://arxiv.org/abs/2204.11454)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c42faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PairwiseUtility = Callable[[str, str], float]\n",
    "\n",
    "def mbr(prompt: str, metric_fn: PairwiseUtility, **generate_kwargs) -> tuple[list[str], list[float]]:\n",
    "    '''\n",
    "    Runs MBR decoding with custom user-defined pairwise utility\n",
    "    '''\n",
    "    codes, response = generate_code(prompt, **generate_kwargs)\n",
    "    pairwise_scores = np.zeros((len(codes), len(codes)))\n",
    "    for i1, code1 in enumerate(codes):\n",
    "        for i2, code2 in enumerate(codes):\n",
    "            if i1 <= i2:\n",
    "                sim = metric_fn(code1, code2)\n",
    "                pairwise_scores[i1, i2] = sim\n",
    "                pairwise_scores[i2, i1] = sim\n",
    "    gains = pairwise_scores.mean(axis=-1)\n",
    "    sorted_indices = np.argsort(gains)[::-1]\n",
    "\n",
    "    gains = [float(gains[i]) for i in sorted_indices]\n",
    "    codes = [codes[i] for i in sorted_indices]\n",
    "    return codes, gains\n",
    "\n",
    "def edit_sim(code1: str, code2: str) -> float:\n",
    "    edit_distance = jellyfish.levenshtein_distance(code1, code2)\n",
    "    return 1 - edit_distance / max(len(code1), len(code2))\n",
    "\n",
    "def make_exec_metric(test_list: list[str) -> PairwiseUtility:\n",
    "    '''\n",
    "    Make pairwise similarity metric for specific example based on that example's test cases\n",
    "    '''\n",
    "    # extract the calls to this example's function\n",
    "    test_func_calls = extract_func_calls(test_list)\n",
    "    \n",
    "    def exec_sim(code1: str, code2: str) -> float:\n",
    "        '''\n",
    "        Runs code1 and code2 on test_func_calls (from the closure of this function)\n",
    "        Returns proportion of func calls with same execution result\n",
    "        '''\n",
    "        result1, result2 = execute_codes([code1, code2], test_func_calls)\n",
    "        n_same = 0\n",
    "        for r1, r2 in zip(result1, result2):\n",
    "            if isinstance(r1, Exception) or isinstance(r2, Exception):\n",
    "                continue\n",
    "            if r1 == r2:\n",
    "                n_same += 1\n",
    "        similarity = n_same / len(test_func_calls)\n",
    "        return similarity\n",
    "    return exec_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6db547e0-a79a-48bb-9f0a-f038c0c8bcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7784882503257926, 0.7784882503257926, 0.7784882503257926, 0.7784882503257926, 0.7784882503257926, 0.7784882503257926, 0.7784882503257926, 0.7714686999833471, 0.7714686999833471, 0.7571552423515044, 0.7571552423515044, 0.7571552423515044, 0.7571552423515044, 0.7571552423515044, 0.7571552423515044, 0.7457562750158142, 0.7255068827231455, 0.7245432998058251, 0.7073885141071726, 0.7067493957561581, 0.6783766468826269, 0.6759921165572798, 0.6759921165572798, 0.6679532163742689, 0.6546167808379094, 0.6527382634190173, 0.6405044385544196, 0.6219316478118512, 0.5927113899384178, 0.4310940143717774]\n",
      "def remove_Occ(s, char):\n",
      "    first_occ = s.find(char)\n",
      "    last_occ = s.rfind(char)\n",
      "    if first_occ != -1 and last_occ != -1:\n",
      "        return s[:first_occ] + s[first_occ+1:last_occ] + s[last_occ+1:]\n",
      "    else:\n",
      "        return s\n"
     ]
    }
   ],
   "source": [
    "# MBR-edit-sim\n",
    "codes, gains = mbr(prompts[0], edit_sim, n=n, temperature=temperature, top_p=top_p, seed=RANDOM_SEED)\n",
    "print(gains)\n",
    "print(codes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dccfed1-77ef-4d8e-b5df-5820c532d047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'failed: substring not found',\n",
       " 'passed']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate MBR-edit-sim\n",
    "execution_results = execute_tests(codes, mbpp[0]['test_list'] + mbpp[0]['challenge_test_list'])\n",
    "[a['result'] for a in execution_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ea1f2-d603-49d4-8822-7abb4641ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBR-exec\n",
    "codes, gains = mbr(prompts[0], make_exec_metric(mbpp[0]['test_list']), n=n, temperature=temperature, top_p=top_p, seed=RANDOM_SEED)\n",
    "print(gains)\n",
    "print(codes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e813c-1c26-46a7-8b43-b4f35443aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_results = execute_tests(codes, mbpp[0]['test_list'])\n",
    "[a['result'] for a in execution_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79633f3b-9f05-4d5d-b6f3-45c9d947bc71",
   "metadata": {},
   "source": [
    "### Self-repair\n",
    "\n",
    "Self-repair (or self-debugging) is a refinement strategy in which the code LLM iteratively repairs its own previously-generated code based on compiler or execution error messages. Here, the __initial generation__ is the code LLM's initial code samples and __feedback__ is derived from the code execution environment. Finally, the model __refines__ its generation by (optionally) explaining the previous turn's error and writing an improved piece of code.\n",
    "\n",
    "\\begin{align*}\n",
    "    y^{(0)}&\\sim g_0(y|x) \\tag{initial generation}\\\\\n",
    "    z^{(t)}&\\sim h(z|x,y^{(<t)},z^{(<t)}) \\tag{feedback}\\\\\n",
    "    y^{(t)}&\\sim g(y|x,y^{(<t)},z^{(\\leq t)}) \\tag{refinement}\n",
    "\\end{align*}\n",
    "\n",
    "Self-repair has been shown to be effective for a variety of code generation tasks [(Chen et al., 2023)](https://arxiv.org/abs/2304.05128). However, for more challenging tasks, it has been found to have some limitations and may not always be more effective than best-of-N [(Olausson et al., 2024)](https://arxiv.org/abs/2306.09896)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b733c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example repair prompt from Olausson et al (https://arxiv.org/abs/2306.09896).\n",
    "repair_template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful programming assistant and an expert Python programmer. \"\n",
    "                   \"You are helping a user write a program to solve a problem. \"\n",
    "                   \"The user has written some code, but it has some errors and is not passing the tests. \"\n",
    "                   \"You will help the user by first giving a concise (at most 2-3 sentences) textual explanation \"\n",
    "                   \"of what is wrong with the code. After you have pointed out what is wrong with the code, \"\n",
    "                   \"you will then generate a fixed version of the program. \"\n",
    "                   \"Put your fixed program within code delimiters, for example: ```\\n# YOUR CODE HERE\\n```.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"### Problem\\n\"\n",
    "                   \"{problem}\\n\\n\"\n",
    "                   \"### Incorrect Code:\\n\"\n",
    "                   \"{code}\\n\\n\"\n",
    "                   \"###Error\\n\"\n",
    "                   \"{error}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def run_repair(code: str, problem: str, error_msg: str, **generate_kwargs) -> str:\n",
    "    '''\n",
    "    Runs a single iteration of refinement given the previous round's code's error message\n",
    "    '''\n",
    "    repair_prompt = copy.deepcopy(repair_template)\n",
    "    repair_prompt[-1]['content'] = repair_prompt[-1]['content'].format(problem=problem, code=code, error=error_msg)\n",
    "\n",
    "    new_code, response = generate_code(repair_prompt, **generate_kwargs)\n",
    "    return new_code, response\n",
    "\n",
    "\n",
    "def self_repair(prompt: str, tests: list[str], max_rounds: int, **generate_kwargs) -> list[str]:\n",
    "    '''\n",
    "    Runs generation with self-repair for a given prompt and tests.\n",
    "    '''\n",
    "    problem = prompt.rsplit(\"\\n\", 1)[0] # remove the additional instruction at the end of the prompt\n",
    "    codes, response = generate_code(prompt, **generate_kwargs)\n",
    "\n",
    "    execution_results = execute_codes(codes, tests)\n",
    "\n",
    "    refine_history = []\n",
    "\n",
    "    refine_history.append((codes, response, copy.deepcopy(execution_results)))\n",
    "    for round in range(max_rounds):\n",
    "        refined_codes = []\n",
    "        refined_responses = []\n",
    "        for code, exec_result in zip(codes, execution_results):\n",
    "            if exec_result['passed']:\n",
    "                refined_codes.append(None)\n",
    "                refined_responses.append(None)\n",
    "                continue\n",
    "\n",
    "            refined_code, refined_response = run_repair(code, exec_result['result'], n=1)\n",
    "            refined_codes.append(refined_code[0])\n",
    "            refined_responses.append(refined_response)\n",
    "        \n",
    "        execution_results = execute_codes(refined_codes, tests)\n",
    "        codes = refined_code\n",
    "        refine_history.append((refined_codes, refined_responses, copy.deepcopy(execution_results)))\n",
    "\n",
    "    return refine_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe8df7-7253-45b3-8df2-c25e5722cdaf",
   "metadata": {},
   "source": [
    "Observe that the model's earlier initial generation did not pass the tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1cbe55d-2118-4a98-a8e7-57a4bd879fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code:\n",
      "def remove_Occ(s, char):\n",
      "    first_occ = s.find(char)\n",
      "    last_occ = s.rfind(char)\n",
      "    \n",
      "    if first_occ != -1:\n",
      "        s = s[:first_occ] + s[first_occ+1:]\n",
      "    if last_occ != -1:\n",
      "        s = s[:last_occ] + s[last_occ+1:]\n",
      "    \n",
      "    return s\n",
      "\n",
      "Result:\n",
      "failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" \n"
     ]
    }
   ],
   "source": [
    "print(\"Code:\\n\" + codes[0] + \"\\n\")\n",
    "print(\"Result:\\n\" + execution_results[0]['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af995509-80ee-438a-9dc7-c5770dc16d7a",
   "metadata": {},
   "source": [
    "Now, let's try an iteration of refinement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6b31cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = prompts[0].rsplit(\"\\n\", 1)[0]\n",
    "repair_code, repair_response = run_repair(codes[0], problem, execution_results[0]['result'], n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27a4fc-7a6f-4223-b38b-0186f1df6103",
   "metadata": {},
   "source": [
    "The new result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ef53b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'task_id': None, 'passed': True, 'result': 'passed', 'completion_id': None}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_tests(repair_code, mbpp[0]['test_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bd79f9-45a4-427c-a88c-f19c98c33aa5",
   "metadata": {},
   "source": [
    "It passes the tests! And the LLM's reasoning for its refinement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34ba95a0-4375-4b2e-8c74-582eac899f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The issue with the code is that it removes the last occurrence of the character before removing the first occurrence. This causes the removal of the first occurrence to shift the index of the last occurrence, resulting in the incorrect output. To fix this, you should remove the last occurrence first and then the first occurrence.\n",
      "\n",
      "```python\n",
      "def remove_Occ(s, char):\n",
      "    last_occ = s.rfind(char)\n",
      "    first_occ = s.find(char)\n",
      "    \n",
      "    if last_occ != -1:\n",
      "        s = s[:last_occ] + s[last_occ+1:]\n",
      "    if first_occ != -1:\n",
      "        s = s[:first_occ] + s[first_occ+1:]\n",
      "    \n",
      "    return s\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(repair_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5a0d9-04ef-4280-a84c-ae77b93fa7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
