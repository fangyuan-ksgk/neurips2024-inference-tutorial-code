{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f51599-188f-40d0-a13f-bb0c360b4179",
   "metadata": {},
   "source": [
    "## Meta-generation case study: Python code generation on MBPP\n",
    "\n",
    "This notebook will demonstrate __best-of-n__,  __minimum Bayes risk decoding__, and __self-repair__ for code generation on the Mostly Basic Python Problems (MBPP) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd8829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from litellm import completion, ModelResponse\n",
    "import datasets\n",
    "import jellyfish # levenshtein distance for MBR utility\n",
    "import numpy as np\n",
    "\n",
    "from typing import Callable, Union\n",
    "from pprint import pprint\n",
    "\n",
    "from mbpp_utils import (\n",
    "    # code execution helpers\n",
    "    check_correctness,\n",
    "    execute_tests,\n",
    "    execute_codes,\n",
    "    \n",
    "    # string processing helpers\n",
    "    make_prompt,\n",
    "    extract_code,\n",
    "    extract_func_calls,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2285d-ae3d-45bb-b9c9-157bdf63e7ed",
   "metadata": {},
   "source": [
    "First, let's load in the MBPP data. MBPP consists of basic algorithmic Python questions, such as the following:\n",
    "\n",
    "```python\n",
    "# Input: Specification\n",
    "\"\"\"\n",
    "Write a python function to remove first and last occurrence of a given character from the string.\n",
    "\"\"\"\n",
    "\n",
    "# Output: Python function\n",
    "def remove_Occ(s,ch): \n",
    "    for i in range(len(s)): \n",
    "        if (s[i] == ch): \n",
    "            s = s[0 : i] + s[i + 1:] \n",
    "            break\n",
    "    for i in range(len(s) - 1,-1,-1):  \n",
    "        if (s[i] == ch): \n",
    "            s = s[0 : i] + s[i + 1:] \n",
    "            break\n",
    "    return s \n",
    "\n",
    "# Public test cases\n",
    "assert remove_Occ(\"hello\",\"l\") == \"heo\"\n",
    "assert remove_Occ(\"abcda\",\"a\") == \"bcd\"\n",
    "assert remove_Occ(\"PHP\",\"P\") == \"H\"\n",
    "\n",
    "# Challenge test cases\n",
    "assert remove_Occ(\"hellolloll\",\"l\") == \"helollol\"\n",
    "assert remove_Occ(\"\",\"l\") == \"\"\n",
    "```\n",
    "\n",
    "For each example, we will only allow the model to have access to the first test case; we will keep the other tests hidden for evaluation only.\n",
    "\n",
    "We'll evaluate outputs based on their execution accuracy on tests. For sampling-based methods, we'll consider the average execution accuracy across all samples (as there is no ranking over samples), while for meta-decoding methods, we'll consider only the execution accuracy of the top-1 returned output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db06b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mbpp data\n",
    "mbpp = datasets.load_dataset(\"mbpp\", split=\"test\")\n",
    "\n",
    "# pprint(\"MBPP example:\")\n",
    "# pprint(mbpp[0])\n",
    "# print()\n",
    "\n",
    "prompts = []\n",
    "\n",
    "# make zero shot prompts for MBPP\n",
    "for example in mbpp:\n",
    "    prompts.append(make_prompt(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b681268-992e-4adf-a2ff-3ade4fd89a1e",
   "metadata": {},
   "source": [
    "### Parallel Sampling\n",
    "\n",
    "First, consider simply sampling a set of outputs from the model in parallel, using temperature & top-$p$ sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a232d434-e494-4cbc-be15-104ceac5e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set some sampling parameters that we'll keep fixed throughtout this demo\n",
    "n = 30             # number of outputs to sample per input\n",
    "temperature = 0.7  # sampling temperature\n",
    "top_p = 0.95       # top-p cutoff\n",
    "RANDOM_SEED = 1618 # fixed random seed to avoid nondeterminism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8836a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for sampling from generator\n",
    "\n",
    "MODEL_NAME = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "def generate_code(prompt: Union[str, list[dict]], **generate_kwargs) -> tuple[list[str], ModelResponse]:\n",
    "    '''\n",
    "    Generates code sample(s) for prompt\n",
    "    Returns list of code samples and OpenAI-like response object.\n",
    "    \n",
    "    This function accepts additional keyword arguments for various generation \n",
    "    parameters supported by the openai api, including temperature, top_p, logprobs, etc.\n",
    "\n",
    "    Note that we use the litellm library for model inference, which is compatible with a \n",
    "    wide array of model frameworks and APIs. See this link for more:\n",
    "    https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs.\n",
    "    While we assume the inference provider is OpenAI, the code in this demo could \n",
    "    also work for other backends with some minor modifications.\n",
    "    '''\n",
    "    if isinstance(prompt, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    else:\n",
    "        assert isinstance(prompt, list) and isinstance(prompt[0], dict)\n",
    "        messages = prompt\n",
    "\n",
    "    response = completion(MODEL_NAME, messages=messages, **generate_kwargs)\n",
    "\n",
    "    # post-processing to extract code from chat model outputs\n",
    "    codes = []\n",
    "    for choice in response.choices:\n",
    "        text = choice.message.content\n",
    "        code = extract_code(text)\n",
    "        codes.append(code)\n",
    "    return codes, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e257eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py:854\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, logging_obj, model, messages, print_verbose, api_key, api_base, acompletion, litellm_params, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 854\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OpenAIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py:790\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, logging_obj, model, messages, print_verbose, api_key, api_base, acompletion, litellm_params, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    778\u001b[0m logging_obj\u001b[38;5;241m.\u001b[39mpre_call(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    780\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mopenai_client\u001b[38;5;241m.\u001b[39mapi_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    786\u001b[0m     },\n\u001b[1;32m    787\u001b[0m )\n\u001b[1;32m    789\u001b[0m headers, response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 790\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_sync_openai_chat_completion_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenai_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m )\n\u001b[1;32m    797\u001b[0m logging_obj\u001b[38;5;241m.\u001b[39mmodel_call_details[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m headers\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py:649\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py:631\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 631\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/openai/_legacy_response.py:356\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    828\u001b[0m validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1277\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m )\n\u001b[0;32m-> 1280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/litellm/main.py:1597\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1591\u001b[0m     logging\u001b[38;5;241m.\u001b[39mpost_call(\n\u001b[1;32m   1592\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   1593\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m   1594\u001b[0m         original_response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m   1595\u001b[0m         additional_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: headers},\n\u001b[1;32m   1596\u001b[0m     )\n\u001b[0;32m-> 1597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/litellm/main.py:1570\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1570\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_chat_completions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m            \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   1584\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[1;32m   1586\u001b[0m \u001b[43m            \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/litellm/llms/OpenAI/openai.py:864\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, logging_obj, model, messages, print_verbose, api_key, api_base, acompletion, litellm_params, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    863\u001b[0m     error_headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(error_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 864\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    865\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mstatus_code, message\u001b[38;5;241m=\u001b[39merror_text, headers\u001b[38;5;241m=\u001b[39merror_headers\n\u001b[1;32m    866\u001b[0m )\n",
      "\u001b[0;31mOpenAIError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Do parallel sampling\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m codes, response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRANDOM_SEED\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m, in \u001b[0;36mgenerate_code\u001b[0;34m(prompt, **generate_kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m     23\u001b[0m     messages \u001b[38;5;241m=\u001b[39m prompt\n\u001b[0;32m---> 25\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# post-processing to extract code from chat model outputs\u001b[39;00m\n\u001b[1;32m     28\u001b[0m codes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/litellm/utils.py:1013\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1010\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1011\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1012\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/litellm/utils.py:903\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m--> 903\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    904\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/litellm/main.py:3009\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   3006\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3008\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 3009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3012\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3015\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2116\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:378\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[1;32m    377\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[1;32m    379\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    380\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    381\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    382\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    383\u001b[0m         litellm_debug_info\u001b[38;5;241m=\u001b[39mextra_information,\n\u001b[1;32m    384\u001b[0m     )\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m503\u001b[39m:\n\u001b[1;32m    386\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# Do parallel sampling\n",
    "codes, response = generate_code(prompts[0], n=n, temperature=temperature, top_p=top_p, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2c2e988-876f-429d-ad2f-72cbaea1011d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean pass@1: 0.5333333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'failed: substring not found',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate generated samples\n",
    "execution_results = execute_tests(codes, mbpp[0]['test_list'])\n",
    "print(\"mean pass@1:\", sum(result['passed'] for result in execution_results) / len(execution_results))\n",
    "[result['result'] for result in execution_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3272e29c-7069-4321-bb37-64317ca10fd1",
   "metadata": {},
   "source": [
    "Of the 30 samples, only 16 pass all the tests. This isn't great -- if we were to randomly sample a program, there would only be a slightly over 50% chance of it being correct :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215866b",
   "metadata": {},
   "source": [
    "### Best-of-$n$\n",
    "\n",
    "$$\\hat{y} = \\arg \\max_{y \\in \\mathcal{Y}} v(y)$$\n",
    "\n",
    "As our first meta-decoding algorithm, we consider best-of-$n$ using a sample's log probability under the generator as its value, i.e. $v(y) = p_\\theta(y\\,|\\,x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74c8508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_of_n_logprob(prompt: str, **generate_kwargs) -> tuple[list[str], list[float]]:\n",
    "    '''\n",
    "    Runs best-of-n generation, using mean sequence logprob as value\n",
    "    Returns list of codes in order of decreasing value\n",
    "    '''\n",
    "    # force these arguments to be passed \n",
    "    generate_kwargs[\"top_logprobs\"] = 1\n",
    "    generate_kwargs[\"logprobs\"] = True\n",
    "    # generate samples\n",
    "    codes, response = generate_code(prompt, **generate_kwargs)\n",
    "    scores = []\n",
    "    # compute mean logprob for each sequence\n",
    "    for choice in response.choices:\n",
    "        logprobs = choice.logprobs['content']\n",
    "        mean_logprob = sum(lp['logprob'] for lp in logprobs) / len(logprobs)\n",
    "        scores.append(mean_logprob)\n",
    "\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    # arrange codes by decreasing mean log probability\n",
    "    scores = [scores[i] for i in sorted_indices]\n",
    "    codes = [codes[i] for i in sorted_indices]\n",
    "    return codes, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f08892dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run best-of-n\n",
    "codes, scores = best_of_n_logprob(prompts[0], n=n, temperature=temperature, top_p=top_p, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9f331e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed: substring not found']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate best-of-n\n",
    "execution_results = execute_tests(codes, mbpp[0]['test_list'])\n",
    "[result['result'] for result in execution_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc04eb9-ed8c-42ed-8b95-bf2ef2f16f77",
   "metadata": {},
   "source": [
    "Even still, the top-1 output is not correct -- worse, the top few candidates are all wrong. As it turns out, generator probability has been found to be a relatively poor proxy for execution accuracy; in contrast, recent work has found greater success using mutual information [(Zhang et al., 2022)](https://arxiv.org/abs/2211.16490) or learned reward models [(Ni et al., 2023)](https://arxiv.org/abs/2302.08468) as value functions for best-of-$n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f8699-5fb7-4f8a-b027-39dacd936144",
   "metadata": {},
   "source": [
    "### Minimum Bayes Risk\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y} &= \\arg \\max_{y' \\in \\mathcal{Y_h}} \\sum_{y \\in \\mathcal{Y_e}} u(y, y')\n",
    "\\end{align*}\n",
    "\n",
    "In this section, we consider minimum Bayes risk. We consider two choices of pairwise utility function: (1) character-level edit similarity and (2) code execution equivalence, referred to as MBR-Exec [(Shi et al., 2022)](https://arxiv.org/abs/2204.11454)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c42faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PairwiseUtility = Callable[[str, str], float]\n",
    "\n",
    "def mbr(prompt: str, metric_fn: PairwiseUtility, **generate_kwargs) -> tuple[list[str], list[float]]:\n",
    "    '''\n",
    "    Runs MBR decoding with custom user-defined pairwise utility\n",
    "    '''\n",
    "    codes, response = generate_code(prompt, **generate_kwargs)\n",
    "    pairwise_scores = np.zeros((len(codes), len(codes)))\n",
    "    for i1, code1 in enumerate(codes):\n",
    "        for i2, code2 in enumerate(codes):\n",
    "            if i1 <= i2:\n",
    "                sim = metric_fn(code1, code2)\n",
    "                pairwise_scores[i1, i2] = sim\n",
    "                pairwise_scores[i2, i1] = sim\n",
    "    gains = pairwise_scores.mean(axis=-1)\n",
    "    sorted_indices = np.argsort(gains)[::-1]\n",
    "\n",
    "    gains = [float(gains[i]) for i in sorted_indices]\n",
    "    codes = [codes[i] for i in sorted_indices]\n",
    "    return codes, gains\n",
    "\n",
    "def edit_sim(code1: str, code2: str) -> float:\n",
    "    edit_distance = jellyfish.levenshtein_distance(code1, code2)\n",
    "    return 1 - edit_distance / max(len(code1), len(code2))\n",
    "\n",
    "def make_exec_metric(test_list: list[str]) -> PairwiseUtility:\n",
    "    '''\n",
    "    Make pairwise similarity metric for specific example based on that example's test cases\n",
    "    '''\n",
    "    # extract the calls to this example's function\n",
    "    test_func_calls = extract_func_calls(test_list)\n",
    "    \n",
    "    def exec_sim(code1: str, code2: str) -> float:\n",
    "        '''\n",
    "        Runs code1 and code2 on test_func_calls (from the closure of this function)\n",
    "        Returns proportion of func calls with same execution result\n",
    "        '''\n",
    "        result1, result2 = execute_codes([code1, code2], test_func_calls)\n",
    "        n_same = 0\n",
    "        for r1, r2 in zip(result1, result2):\n",
    "            if isinstance(r1, Exception) or isinstance(r2, Exception):\n",
    "                continue\n",
    "            if r1 == r2:\n",
    "                n_same += 1\n",
    "        similarity = n_same / len(test_func_calls)\n",
    "        return similarity\n",
    "    return exec_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6db547e0-a79a-48bb-9f0a-f038c0c8bcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7784882503257926, 0.7784882503257926, 0.7784882503257926, 0.7784882503257926, 0.7784882503257926, 0.7784882503257926, 0.7784882503257926, 0.7714686999833471, 0.7714686999833471, 0.7571552423515044, 0.7571552423515044, 0.7571552423515044, 0.7571552423515044, 0.7571552423515044, 0.7571552423515044, 0.7457562750158142, 0.7255068827231455, 0.7245432998058251, 0.7073885141071726, 0.7067493957561581, 0.6783766468826269, 0.6759921165572798, 0.6759921165572798, 0.6679532163742689, 0.6546167808379094, 0.6527382634190173, 0.6405044385544196, 0.6219316478118512, 0.5927113899384178, 0.4310940143717774]\n",
      "def remove_Occ(s, char):\n",
      "    first_occ = s.find(char)\n",
      "    last_occ = s.rfind(char)\n",
      "    if first_occ != -1 and last_occ != -1:\n",
      "        return s[:first_occ] + s[first_occ+1:last_occ] + s[last_occ+1:]\n",
      "    else:\n",
      "        return s\n"
     ]
    }
   ],
   "source": [
    "# MBR-edit-sim\n",
    "codes, gains = mbr(prompts[0], edit_sim, n=n, temperature=temperature, top_p=top_p, seed=RANDOM_SEED)\n",
    "print(gains)\n",
    "print(codes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dccfed1-77ef-4d8e-b5df-5820c532d047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'passed',\n",
       " 'failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" ',\n",
       " 'passed',\n",
       " 'failed: substring not found',\n",
       " 'passed']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate MBR-edit-sim\n",
    "execution_results = execute_tests(codes, mbpp[0]['test_list'])\n",
    "[a['result'] for a in execution_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ea1f2-d603-49d4-8822-7abb4641ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBR-exec\n",
    "codes, gains = mbr(prompts[0], make_exec_metric(mbpp[0]['test_list'][:1]), n=n, temperature=temperature, top_p=top_p, seed=RANDOM_SEED)\n",
    "print(gains)\n",
    "print(codes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e813c-1c26-46a7-8b43-b4f35443aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_results = execute_tests(codes, mbpp[0]['test_list'])\n",
    "[a['result'] for a in execution_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79633f3b-9f05-4d5d-b6f3-45c9d947bc71",
   "metadata": {},
   "source": [
    "### Self-repair\n",
    "\n",
    "Self-repair (or self-debugging) is a refinement strategy in which the code LLM iteratively repairs its own previously-generated code based on compiler or execution error messages. Here, the __initial generation__ is the code LLM's initial code samples and __feedback__ is derived from the code execution environment. Finally, the model __refines__ its generation by (optionally) explaining the previous turn's error and writing an improved piece of code.\n",
    "\n",
    "\\begin{align*}\n",
    "    y^{(0)}&\\sim g_0(y|x) \\tag{initial generation}\\\\\n",
    "    z^{(t)}&\\sim h(z|x,y^{(<t)},z^{(<t)}) \\tag{feedback}\\\\\n",
    "    y^{(t)}&\\sim g(y|x,y^{(<t)},z^{(\\leq t)}) \\tag{refinement}\n",
    "\\end{align*}\n",
    "\n",
    "Self-repair has been shown to be effective for a variety of code generation tasks [(Chen et al., 2023)](https://arxiv.org/abs/2304.05128). However, for more challenging tasks, it has been found to have some limitations and may not always be more effective than best-of-N [(Olausson et al., 2024)](https://arxiv.org/abs/2306.09896)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b733c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example repair prompt from Olausson et al (https://arxiv.org/abs/2306.09896).\n",
    "repair_template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful programming assistant and an expert Python programmer. \"\n",
    "                   \"You are helping a user write a program to solve a problem. \"\n",
    "                   \"The user has written some code, but it has some errors and is not passing the tests. \"\n",
    "                   \"You will help the user by first giving a concise (at most 2-3 sentences) textual explanation \"\n",
    "                   \"of what is wrong with the code. After you have pointed out what is wrong with the code, \"\n",
    "                   \"you will then generate a fixed version of the program. \"\n",
    "                   \"Put your fixed program within code delimiters, for example: ```\\n# YOUR CODE HERE\\n```.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"### Problem\\n\"\n",
    "                   \"{problem}\\n\\n\"\n",
    "                   \"### Incorrect Code:\\n\"\n",
    "                   \"{code}\\n\\n\"\n",
    "                   \"###Error\\n\"\n",
    "                   \"{error}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def run_repair(code: str, problem: str, error_msg: str, **generate_kwargs) -> str:\n",
    "    '''\n",
    "    Runs a single iteration of refinement given the previous round's code's error message\n",
    "    '''\n",
    "    repair_prompt = copy.deepcopy(repair_template)\n",
    "    repair_prompt[-1]['content'] = repair_prompt[-1]['content'].format(problem=problem, code=code, error=error_msg)\n",
    "\n",
    "    new_code, response = generate_code(repair_prompt, **generate_kwargs)\n",
    "    return new_code, response\n",
    "\n",
    "\n",
    "def self_repair(prompt: str, tests: list[str], max_rounds: int, **generate_kwargs) -> list[str]:\n",
    "    '''\n",
    "    Runs generation with self-repair for a given prompt and tests.\n",
    "    '''\n",
    "    problem = prompt.rsplit(\"\\n\", 1)[0] # remove the additional instruction at the end of the prompt\n",
    "    codes, response = generate_code(prompt, **generate_kwargs)\n",
    "\n",
    "    execution_results = execute_codes(codes, tests)\n",
    "\n",
    "    refine_history = []\n",
    "\n",
    "    refine_history.append((codes, response, copy.deepcopy(execution_results)))\n",
    "    for round in range(max_rounds):\n",
    "        refined_codes = []\n",
    "        refined_responses = []\n",
    "        for code, exec_result in zip(codes, execution_results):\n",
    "            if exec_result['passed']:\n",
    "                refined_codes.append(None)\n",
    "                refined_responses.append(None)\n",
    "                continue\n",
    "\n",
    "            refined_code, refined_response = run_repair(code, exec_result['result'], n=1)\n",
    "            refined_codes.append(refined_code[0])\n",
    "            refined_responses.append(refined_response)\n",
    "        \n",
    "        execution_results = execute_codes(refined_codes, tests)\n",
    "        codes = refined_code\n",
    "        refine_history.append((refined_codes, refined_responses, copy.deepcopy(execution_results)))\n",
    "\n",
    "    return refine_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe8df7-7253-45b3-8df2-c25e5722cdaf",
   "metadata": {},
   "source": [
    "Observe that the model's earlier initial generation did not pass the tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1cbe55d-2118-4a98-a8e7-57a4bd879fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code:\n",
      "def remove_Occ(s, char):\n",
      "    first_occ = s.find(char)\n",
      "    last_occ = s.rfind(char)\n",
      "    \n",
      "    if first_occ != -1:\n",
      "        s = s[:first_occ] + s[first_occ+1:]\n",
      "    if last_occ != -1:\n",
      "        s = s[:last_occ] + s[last_occ+1:]\n",
      "    \n",
      "    return s\n",
      "\n",
      "Result:\n",
      "failed:  assert remove_Occ(\"hello\",\"l\") == \"heo\" \n"
     ]
    }
   ],
   "source": [
    "print(\"Code:\\n\" + codes[0] + \"\\n\")\n",
    "print(\"Result:\\n\" + execution_results[0]['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af995509-80ee-438a-9dc7-c5770dc16d7a",
   "metadata": {},
   "source": [
    "Now, let's try an iteration of refinement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6b31cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = prompts[0].rsplit(\"\\n\", 1)[0]\n",
    "repair_code, repair_response = run_repair(codes[0], problem, execution_results[0]['result'], n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27a4fc-7a6f-4223-b38b-0186f1df6103",
   "metadata": {},
   "source": [
    "The new result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ef53b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'task_id': None, 'passed': True, 'result': 'passed', 'completion_id': None}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_tests(repair_code, mbpp[0]['test_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bd79f9-45a4-427c-a88c-f19c98c33aa5",
   "metadata": {},
   "source": [
    "It passes the tests! And the LLM's reasoning for its refinement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34ba95a0-4375-4b2e-8c74-582eac899f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The issue with the code is that it removes the last occurrence of the character before removing the first occurrence. This causes the removal of the first occurrence to shift the index of the last occurrence, resulting in the incorrect output. To fix this, you should remove the last occurrence first and then the first occurrence.\n",
      "\n",
      "```python\n",
      "def remove_Occ(s, char):\n",
      "    last_occ = s.rfind(char)\n",
      "    first_occ = s.find(char)\n",
      "    \n",
      "    if last_occ != -1:\n",
      "        s = s[:last_occ] + s[last_occ+1:]\n",
      "    if first_occ != -1:\n",
      "        s = s[:first_occ] + s[first_occ+1:]\n",
      "    \n",
      "    return s\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(repair_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5a0d9-04ef-4280-a84c-ae77b93fa7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
